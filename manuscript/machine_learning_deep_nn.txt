Deep Learning Neural Networks
=============================

One limitation of back propagation neural networks seen in the last chapter is that they are limited to the number of neuron layers that can be efficiently trained. If you experimented with the sample back propagation code then you may have noticed that it took a lot longer to train a network with two hidden layers compared to the training time for a network with only one hidden layer.

The problem with back propagation networks is that as error gradients are back propagated through the network toward the input layer, the gradients get smaller and smaller. The effect is that it can take a lot of time to train back propagation networks with many hidden layers. We will see in the next chapter on Deep Learning how this problem can be solved and deep networks (i.e., networks with many hidden layers) can be trained.

I became interested in deep learning neural networks when I took Geoffrey Hinton's Neural Network class (a Coursera class, taken summer of 2012).

TBD discus optimization trick

TBD: find an existing Java lib for this, or write up some example code

Most open source deep learning projects are written either in Octave (or the commercial Matlab product) or Python. If you like to use Octave then check out the [DeepLearningToolbox project](https://github.com/rasmusbergpalm/DeepLearnToolbox) and if you use Python then look at one of the Theano based projects like [ORB](https://github.com/benanne/morb).

